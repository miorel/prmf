2010-06-04	Rodrigo Salazar <rsala004@ufl.edu>

	* threaded_spider.pl: Erased lots of code, added many comments as to how the threading will be approached...
	* basic_spider.pl: made minor change to the mini-hashing if condition.
	
2010-06-04	Rodrigo Salazar <rsala004@ufl.edu>

	Overall refinement of the basic spidering method!
	
	* util.pm: Fixed problem with links to subdirectories.
	* settings.pm: Created, stores basic settings such as whether threading / hashing is enabled.
	* basic_spider.pl: Fixed problem that was requiring checking hashes, hashing no longer 100% necessary .. but still
	somewhat helpful (Reason why the settings file was created). Also removed several useless variables. Added warning
	and strict packages.
	* threaded_spider.pl: Added warning and strict packages.
	
2010-06-02	Rodrigo Salazar <rsala004@ufl.edu>

	* basic_spider.pl: Revamped the basic_spider to help build off of it for work on the threaded version..
	Now downloads the header info before downloading entire page to make sure its not an old page.
	The subroutine now uses a stack to manage the links instead of recursion.
	
2010-06-02	Rodrigo Salazar <rsala004@ufl.edu>

	* pattern.pm: Added new pattern /#/ to ignore pages linking to themselves.. doesn't seem to work?
	* util.pm: Added clause to strip www subdomain, which was causing websites to be redownloaded.
	
2010-06-02	Rodrigo Salazar <rsala004@ufl.edu>

	* renamed all files to lowercase type, project reorganized completely, much cleaner now.
	* craigcrawler.pl: moved all code that has to do with spidering to other files.
	* pattern.pm: created to hold patterns and get methods.
	* threaded_spider.pl: now contains subroutines for threaded crawling, still not working.
	* util.pm: now gets the patterns it needs through get methods.
	* basic_spider.pl: seems to be broken now with the new code organization, it is not sifting through the
	  patterns correctly, might have something to do with the util.pm or pattern.pm files not functioning as intended.
	
2010-05-30	Rodrigo Salazar <rsala004@ufl.edu>

	* Craigcrawler.pl: Moved some code to other files, still needs more work.
	* Basic_Spider.pl: Created file, still empty.
	* Threaded_Spider.pl: Created file, still empty.
	* Util.pm: Moved validate_link and clean_link methods here.
	
2010-05-30	Rodrigo Salazar <rsala004@ufl.edu>

	* craigcrawler.pl: Created basic code for the multithreaded spider, it currently locks up very quickly.
	
2010-05-30	Rodrigo Salazar <rsala004@ufl.edu>

	* craigcrawler.pl: Wrote up a very basic plan for how the threads will download and update the links list.
	
2010-05-29	Rodrigo Salazar <rsala004@ufl.edu>

	* craigcrawler.pl: Added possible solution to an infinite loop problem (caused by unique links leading to old pages).
	Solved with expensive solution though..using an md5 hash, I do think its possible to solve this problem by examining
	craigslist website more closely, but I can't find any problem links at this moment.. the infinite loops otherwise happens
	hours into the program's crawl which makes it hard to find the exact problem..for now this solution might do well.
	
2010-05-29	Rodrigo Salazar <rsala004@ufl.edu>

	* craigcrawler.pl: Fixed bug with multiple slashes appearing. Spider no longer visits ad-listings so to scan faster.
	All ad-links and ad-descriptions are stored in a separate table now. Added a basic pattern to match ad listings.
	
2010-05-28	Rodrigo Salazar <rsala004@ufl.edu>

	* craigcrawler.pl: Deleted 'pattern' and 'pattern2', decided to go with a general link pattern and
	then sieve it down from there. Program seems to work much better now, possibly as intended? needs debugging.
	Cleaned up the code significantly and major improvements made!

2010-05-27	Rodrigo Salazar <rsala004@ufl.edu>

	* craigcrawler.pl: Cleaned up the code some more, possibly got 2nd pattern working, but still overall broken.
	Code runs without errors, but not as intended .. (Ad pages still not being reached).
	Added one more pattern to 'bad_pattern' array. 
	Need to rethink what is going on... Pattern2 seems redundant with Pattern1 (seeing this as I write changelog)


2010-05-27	Rodrigo Salazar <rsala004@ufl.edu>

	* craigcrawler.pl: Changed main pattern, added additional pattern for directories, added bad_pattern array since
	I was not able to combine them into 1 super-pattern. Bug created somehow .. if spider reaches max recursion depth it freezes.
	Still not able to get spider to go all the way down to listings pages.
	
2010-05-24	Rodrigo Salazar <rsala004@ufl.edu>

	* craigcrawler.pl: Erased dont_spider method because I decided its a horrible way to check for bad links.
	It might be better just to be more specific with our original pattern.. Added a list of problems to tackle!
	
2010-05-24	Rodrigo Salazar <rsala004@ufl.edu>

	* craigcrawler.pl: Added a statement to remove leading slashes in directory links.
	Attempted to change one of the 'bad' patterns, currently a broken pattern which doesn't 'compile'.
	
2010-05-24	Rodrigo Salazar <rsala004@ufl.edu>

	* craigcrawler.pl: Made some progress on what to spider and what not to spider, but broke the spider in the process.
	Added some very messy code.
	
2010-05-23	Rodrigo Salazar <rsala004@ufl.edu>

	* craigcrawler.pl: Implemented basic spider method, which works somewhat!
	Problems: There are a TON of pages! All of 1 city took 15 minutes.
			  Script terminated itself after completing it's first city.
	* ChangeLog: organized changelog
	
2010-05-21  Miorel-Lucian Palii <mlpalii@gmail.com>

	* craigcrawler.pl: Answered in the comments a question Rodrigo posted in the
	  comments.

2010-05-21  Rodrigo Salazar <rsala004@ufl.edu>

	* craigcrawler.pl: Set a layout for the first couple methods we will need to
	  get started with this project, used pseudo-pseudo-code to describe the
	  methods.

2010-05-21  Rodrigo Salazar <rsala004@ufl.edu>

	* craigcrawler.pl: Added a couple lines of code, need to think about how we
	  are going to get all the interesting links from the page or whether we
	  should just pull every link and spider the entire website, checking each
	  to see if its a person listing or not. Will discuss later!

2010-05-21  Rodrigo Salazar <rsala004@ufl.edu>

	* .project: Updated CraigCrawler with Eclipse project settings (EPIC Perl
	plugin).

2010-05-20  Miorel-Lucian Palii <mlpalii@gmail.com>

	* craigcrawler.pl: Updated file to use warnings and strict pragmas.
	* COPYING: Added license to project.

2010-05-20  Rodrigo Salazar <rsala004@ufl.edu>

	* craigcrawler.pl: Added a Perl file to work off of for the new
	  craigslist bot project!
